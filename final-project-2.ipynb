{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6212341,"sourceType":"datasetVersion","datasetId":3532416},{"sourceId":7898378,"sourceType":"datasetVersion","datasetId":4638336},{"sourceId":8233289,"sourceType":"datasetVersion","datasetId":4882904},{"sourceId":8234431,"sourceType":"datasetVersion","datasetId":4883787},{"sourceId":8234670,"sourceType":"datasetVersion","datasetId":4883962}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-crf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FRqEZLmChWT6","outputId":"a204b01b-17d0-4813-ce1e-cd7da6d98fa8","execution":{"iopub.status.busy":"2024-04-26T06:05:36.030370Z","iopub.execute_input":"2024-04-26T06:05:36.031025Z","iopub.status.idle":"2024-04-26T06:05:49.913402Z","shell.execute_reply.started":"2024-04-26T06:05:36.030992Z","shell.execute_reply":"2024-04-26T06:05:49.912415Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pytorch-crf\n  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\nDownloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\nInstalling collected packages: pytorch-crf\nSuccessfully installed pytorch-crf-0.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import RobertaModel\nfrom torchcrf import CRF\n\nclass NERModel(nn.Module):\n    def __init__(self, num_labels, hidden_size):\n        super(NERModel, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(hidden_size, num_labels)\n        self.crf = CRF(num_labels, batch_first=True)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = outputs.last_hidden_state\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n        if labels is not None:\n#             print(logits.size())\n#             print(labels.size())\n#             print(attention_mask.size())\n            loss = -self.crf(logits, labels, mask=attention_mask.byte(), reduction='mean')\n            return loss\n        else:\n            return logits\n","metadata":{"id":"2tkaz80-hWUI","execution":{"iopub.status.busy":"2024-04-26T06:06:54.269403Z","iopub.execute_input":"2024-04-26T06:06:54.269843Z","iopub.status.idle":"2024-04-26T06:06:54.283980Z","shell.execute_reply.started":"2024-04-26T06:06:54.269809Z","shell.execute_reply":"2024-04-26T06:06:54.282973Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9NaGlkThbeG","outputId":"1341cde6-8279-4781-9147-2a4cabea41c4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/drive\n"}]},{"cell_type":"code","source":"label_map = {\n    'B-Facility': 1,\n    'I-Facility': 2,\n    'B-OtherLOC': 3,\n    'I-OtherLOC': 4,\n    'B-HumanSettlement': 5,\n    'I-HumanSettlement': 6,\n    'B-Station': 7,\n    'I-Station': 8,\n    'B-VisualWork': 9,\n    'I-VisualWork': 10,\n    'B-MusicalWork': 11,\n    'I-MusicalWork': 12,\n    'B-WrittenWork': 13,\n    'I-WrittenWork': 14,\n    'B-ArtWork': 15,\n    'I-ArtWork': 16,\n    'B-Software': 17,\n    'I-Software': 18,\n    'B-MusicalGRP': 19,\n    'I-MusicalGRP': 20,\n    'B-PublicCorp': 21,\n    'I-PublicCorp': 22,\n    'B-PrivateCorp': 23,\n    'I-PrivateCorp': 24,\n    'B-AerospaceManufacturer': 25,\n    'I-AerospaceManufacturer': 26,\n    'B-SportsGRP': 27,\n    'I-SportsGRP': 28,\n    'B-CarManufacturer': 29,\n    'I-CarManufacturer': 30,\n    'B-ORG': 31,\n    'I-ORG': 32,\n    'B-Scientist': 33,\n    'I-Scientist': 34,\n    'B-Artist': 35,\n    'I-Artist': 36,\n    'B-Athlete': 37,\n    'I-Athlete': 38,\n    'B-Politician': 39,\n    'I-Politician': 40,\n    'B-Cleric': 41,\n    'I-Cleric': 42,\n    'B-SportsManager': 43,\n    'I-SportsManager': 44,\n    'B-OtherPER': 45,\n    'I-OtherPER': 46,\n    'B-Clothing': 47,\n    'I-Clothing': 48,\n    'B-Vehicle': 49,\n    'I-Vehicle': 50,\n    'B-Food': 51,\n    'I-Food': 52,\n    'B-Drink': 53,\n    'I-Drink': 54,\n    'B-OtherPROD': 55,\n    'I-OtherPROD': 56,\n    'B-Medication/Vaccine': 57,\n    'I-Medication/Vaccine': 58,\n    'B-MedicalProcedure': 59,\n    'I-MedicalProcedure': 60,\n    'B-AnatomicalStructure': 61,\n    'I-AnatomicalStructure': 62,\n    'B-Symptom': 63,\n    'I-Symptom': 64,\n    'B-Disease': 65,\n    'I-Disease': 66,\n    'O': 67\n}\n","metadata":{"id":"T-_M8nJShWUL","execution":{"iopub.status.busy":"2024-04-26T06:06:59.097653Z","iopub.execute_input":"2024-04-26T06:06:59.098378Z","iopub.status.idle":"2024-04-26T06:06:59.108006Z","shell.execute_reply.started":"2024-04-26T06:06:59.098349Z","shell.execute_reply":"2024-04-26T06:06:59.107005Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"new_label_map = {key: label - 1 for key, label in label_map.items()}","metadata":{"id":"2BvMavbyhWUN","execution":{"iopub.status.busy":"2024-04-26T06:06:59.999019Z","iopub.execute_input":"2024-04-26T06:06:59.999772Z","iopub.status.idle":"2024-04-26T06:07:00.004207Z","shell.execute_reply.started":"2024-04-26T06:06:59.999739Z","shell.execute_reply":"2024-04-26T06:07:00.003174Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"label_map=new_label_map","metadata":{"id":"PWApvAlOhWUP","execution":{"iopub.status.busy":"2024-04-26T06:07:00.387607Z","iopub.execute_input":"2024-04-26T06:07:00.388458Z","iopub.status.idle":"2024-04-26T06:07:00.392418Z","shell.execute_reply.started":"2024-04-26T06:07:00.388429Z","shell.execute_reply":"2024-04-26T06:07:00.391348Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    input_ids_batch, attention_mask_batch, label_ids_batch = zip(*batch)\n#     print(input_ids_batch)\n#     print(attention_mask_batch)\n#     print(label_ids_batch)\n    # Pad sequences to the maximum length in the batch\n    input_ids_batch = pad_sequence(input_ids_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n    attention_mask_batch = pad_sequence(attention_mask_batch, batch_first=True, padding_value=0)  # 0 is used as the default attention mask value\n    label_ids_batch = pad_sequence(label_ids_batch, batch_first=True, padding_value=-100)  # -100 is used as the default label ID value for padding\n\n    return input_ids_batch, attention_mask_batch, label_ids_batch\n","metadata":{"id":"div-OL4ghWUS","execution":{"iopub.status.busy":"2024-04-26T06:07:03.572777Z","iopub.execute_input":"2024-04-26T06:07:03.573474Z","iopub.status.idle":"2024-04-26T06:07:03.579668Z","shell.execute_reply.started":"2024-04-26T06:07:03.573446Z","shell.execute_reply":"2024-04-26T06:07:03.578662Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_preserve_labels(tokens, la):\n    tokenized_sentence = []\n    labels = []\n\n    for word, label in zip(sentence, text_labels):\n        tokenized_word = tokenizer.tokenize(word)\n        n_subwords = len(tokenized_word)\n        tokenized_sentence.extend(tokenized_word)\n        labels.extend([label] * n_subwords)\n\n    return tokenized_sentence, labels","metadata":{"id":"G_nZcdT5hWUW","execution":{"iopub.status.busy":"2024-04-26T06:07:06.398805Z","iopub.execute_input":"2024-04-26T06:07:06.399161Z","iopub.status.idle":"2024-04-26T06:07:06.404653Z","shell.execute_reply.started":"2024-04-26T06:07:06.399133Z","shell.execute_reply":"2024-04-26T06:07:06.403737Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom transformers import RobertaTokenizer\n\nclass MultiCoNERDataset(Dataset):\n    def __init__(self, file_path, tokenizer, regime=\"train\"):\n        self.samples = self._read_data(file_path)\n        self.tokenizer = tokenizer\n        self.regime = regime\n\n    def _read_data(self, file_path):\n        samples = []\n        with open(file_path, 'r', encoding='utf-8') as file:\n            lines = file.readlines()\n            sample = {\"tokens\": [], \"labels\": []}\n            for line in lines:\n                line = line.strip()\n                if line.startswith(\"# id\"):\n                    if sample[\"tokens\"]:\n                        samples.append(sample)\n                        sample = {\"tokens\": [], \"labels\": []}\n                elif line:\n                    parts = line.split()\n                    token, label = parts[0], parts[-1]\n                    sample[\"tokens\"].append(token)\n                    sample[\"labels\"].append(label)\n            if sample[\"tokens\"]:\n                samples.append(sample)\n        return samples\n\n    def __len__(self):\n        if self.regime ==\"validation\":\n          return 200\n        elif self.regime ==\"test\":\n          return 500\n        else:\n          return len(self.samples)\n\n\n    def __getitem__(self, idx):\n        global label_map\n        sample = self.samples[idx]\n        tokens = sample[\"tokens\"]\n        labels = sample[\"labels\"]\n        # Tokenize and encode inputs\n        inputs = self.tokenizer(tokens, return_tensors='pt',padding=\"max_length\", truncation=True, is_split_into_words=True)\n        input_ids = inputs['input_ids'].squeeze(0)\n        attention_mask = inputs['attention_mask'].squeeze(0)\n        label_ids = [label_map[label] for label in labels]\n        label_ids = label_ids + [-1] * (self.tokenizer.model_max_length - len(label_ids))\n#         print(label_ids)\n#         print(\"TENSOR SIZE:\")\n#         print(input_ids.size())\n#         print(attention_mask.size())\n#         print(len(label_ids))\n#         print(label_ids)\n        return input_ids, attention_mask, torch.tensor(label_ids)","metadata":{"id":"jSFaAEGPhWUa","execution":{"iopub.status.busy":"2024-04-26T06:07:06.916084Z","iopub.execute_input":"2024-04-26T06:07:06.916441Z","iopub.status.idle":"2024-04-26T06:07:06.932221Z","shell.execute_reply.started":"2024-04-26T06:07:06.916415Z","shell.execute_reply":"2024-04-26T06:07:06.931301Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8R5TV5OChWUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer, RobertaConfig\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nPATH=\"/content/drive/MyDrive/Colab Notebooks/NLP/multiconer2023\"\n\n# Define function for training and checkpointing\ndef train_and_checkpoint(model, language, train_loader, val_loader):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    best_val_loss = float('inf')\n    best_model_state_dict = None\n    for epoch in range(5):  # Example: 3 epochs\n        model.train()\n        total_loss = 0\n        for batch in train_loader:\n            input_ids, attention_mask, labels = batch\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = model(input_ids, attention_mask, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        avg_train_loss = total_loss / len(train_loader)\n        model.eval()\n        total_val_loss = 0\n        all_preds = []\n        all_labels = []\n        for batch in val_loader:\n            input_ids, attention_mask, labels = batch\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            with torch.no_grad():\n                loss = model(input_ids, attention_mask, labels)\n                logits = model(input_ids, attention_mask)\n                preds = torch.argmax(logits, dim=2)\n                all_preds.extend(preds.cpu().numpy().flatten())\n                all_labels.extend(labels.cpu().numpy().flatten())\n            total_val_loss += loss.item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_precision = precision_score(all_labels, all_preds, average='micro')\n        val_recall = recall_score(all_labels, all_preds, average='micro')\n        val_f1 = f1_score(all_labels, all_preds, average='micro')\n        print(f\"Epoch {epoch+1}/{5}, Language: {language}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n        print(f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}\")\n\n        # Check if current model is the best performing one\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            best_model_state_dict = model.state_dict()\n\n    # Save the best performing model checkpoint\n    torch.save(best_model_state_dict, os.path.join(f\"/kaggle/working/{language}_best_model.pth\"))\n\n# Define function for testing\ndef test(language, test_loader):\n    config = XLMRobertaConfig.from_pretrained(\"FacebookAI/roberta-base\")\n    num_labels = len(label_map)\n    print(num_labels, config.hidden_size)\n    # Load model checkpoint\n    model = NERModel(num_labels, config.hidden_size)\n    freeze_weignts(model)\n    model.load_state_dict(torch.load(f\"/kaggle/working/{language}_best_model.pth\"))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n\n    # Lists to store true labels and predicted labels\n    all_true_labels = []\n    all_predicted_labels = []\n\n    # Testing loop\n    for batch in test_loader:\n        input_ids, attention_mask, labels = batch\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        with torch.no_grad():\n            # Pass data through model\n            logits = model(input_ids, attention_mask)\n            predictions = torch.argmax(logits, dim=2)\n            # Convert labels and predictions to numpy arrays\n            true_labels = labels.cpu().numpy().flatten()\n            predicted_labels = predictions.cpu().numpy().flatten()\n            # Append true and predicted labels to lists\n            all_true_labels.extend(true_labels)\n            all_predicted_labels.extend(predicted_labels)\n\n    # Calculate F1 score\n    test_f1 = f1_score(all_true_labels, all_predicted_labels, average='micro')\n    print(f\"Test F1 Score for {language}: {test_f1:.4f}\")","metadata":{"id":"TAJX6lhChWUe","execution":{"iopub.status.busy":"2024-04-26T06:07:11.161886Z","iopub.execute_input":"2024-04-26T06:07:11.162259Z","iopub.status.idle":"2024-04-26T06:07:11.742119Z","shell.execute_reply.started":"2024-04-26T06:07:11.162231Z","shell.execute_reply":"2024-04-26T06:07:11.741303Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# os.path.join(PATH,language+\"_best_model.pth\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"j3maPSQHoTJm","outputId":"3aee855a-1135-443f-d2dd-c629cf622839","execution":{"iopub.status.busy":"2024-04-26T06:07:12.261993Z","iopub.execute_input":"2024-04-26T06:07:12.262359Z","iopub.status.idle":"2024-04-26T06:07:12.266506Z","shell.execute_reply.started":"2024-04-26T06:07:12.262331Z","shell.execute_reply":"2024-04-26T06:07:12.265536Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\nmodel_layers=[\"roberta.encoder.layer.9.attention.self.query.weight\",\"roberta.encoder.layer.9.attention.self.query.bias\",\n              \"roberta.encoder.layer.9.attention.self.key.weight\",\"roberta.encoder.layer.9.attention.self.key.bias\",\n              \"roberta.encoder.layer.9.attention.self.value.weight\",\"roberta.encoder.layer.9.attention.self.value.bias\",\n              \"roberta.encoder.layer.9.attention.output.dense.weight\",\"roberta.encoder.layer.9.attention.output.dense.bias\",\n              \"roberta.encoder.layer.9.attention.output.LayerNorm.weight\",\"roberta.encoder.layer.9.attention.output.LayerNorm.bias\",\n              \"roberta.encoder.layer.9.intermediate.dense.weight\",\"roberta.encoder.layer.9.intermediate.dense.bias\",\n              \"roberta.encoder.layer.9.output.dense.weight\",\"roberta.encoder.layer.9.output.dense.bias\",\n              \"roberta.encoder.layer.9.output.LayerNorm.weight\",\"roberta.encoder.layer.9.output.LayerNorm.bias\",\n              \"roberta.encoder.layer.10.attention.self.query.weight\",\"roberta.encoder.layer.10.attention.self.query.bias\",\n              \"roberta.encoder.layer.10.attention.self.key.weight\",\"roberta.encoder.layer.10.attention.self.key.bias\",\n              \"roberta.encoder.layer.10.attention.self.value.weight\", \"roberta.encoder.layer.10.attention.self.value.bias\",\n              \"roberta.encoder.layer.10.attention.output.dense.weight\",\"roberta.encoder.layer.10.attention.output.dense.bias\",\n\"roberta.encoder.layer.10.attention.output.LayerNorm.weight\",\n\"roberta.encoder.layer.10.attention.output.LayerNorm.bias\",\n\"roberta.encoder.layer.10.intermediate.dense.weight\",\n\"roberta.encoder.layer.10.intermediate.dense.bias\",\n\"roberta.encoder.layer.10.output.dense.weight\",\n\"roberta.encoder.layer.10.output.dense.bias\",\n\"roberta.encoder.layer.10.output.LayerNorm.weight\",\n\"roberta.encoder.layer.10.output.LayerNorm.bias\",\n\"roberta.encoder.layer.11.attention.self.query.weight\",\n\"roberta.encoder.layer.11.attention.self.query.bias\",\n\"roberta.encoder.layer.11.attention.self.key.weight\",\n\"roberta.encoder.layer.11.attention.self.key.bias\",\n\"roberta.encoder.layer.11.attention.self.value.weight\",\n\"roberta.encoder.layer.11.attention.self.value.bias\",\n      \"roberta.encoder.layer.11.attention.output.dense.weight\",\n\"roberta.encoder.layer.11.attention.output.dense.bias\",\n\"roberta.encoder.layer.11.attention.output.LayerNorm.weight\",\n\"roberta.encoder.layer.11.attention.output.LayerNorm.bias\",\n\"roberta.encoder.layer.11.intermediate.dense.weight\",\n\"roberta.encoder.layer.11.intermediate.dense.bias\",\n\"roberta.encoder.layer.11.output.dense.weight\",\n\"roberta.encoder.layer.11.output.dense.bias\",\n\"roberta.encoder.layer.11.output.LayerNorm.weight\",\n\"roberta.encoder.layer.11.output.LayerNorm.bias\",\n\"roberta.pooler.dense.weight\",\n \"roberta.pooler.dense.bias\",\n\"classifier.weight\",\n \"classifier.bias\",\n\"crf.start_transitions\",\n\"crf.end_transitions\",\n\"crf.transitions\"]\n\n\ndef freeze_weignts(model):\n    print(\"freezing weignts\")\n    for name, para in model.named_parameters():\n        if name not in model_layers:\n             para.requires_grad = False","metadata":{"id":"6EM_erBrhWUh","execution":{"iopub.status.busy":"2024-04-26T06:07:13.296305Z","iopub.execute_input":"2024-04-26T06:07:13.297138Z","iopub.status.idle":"2024-04-26T06:07:13.305865Z","shell.execute_reply.started":"2024-04-26T06:07:13.297106Z","shell.execute_reply":"2024-04-26T06:07:13.304903Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import os\n\nPATH=\"/kaggle/input/multiconer/multiconer2023\"\n# Example usage\nfor language in os.listdir(PATH):\n    print(\"+\"*20)\n    locale = language.split(\"-\")[0].lower()\n    if len(locale)==2 and locale in [\"de\", \"hi\", \"fr\"]:\n#         dev_path = PATH +\"/\"+language+\"/\"+locale+\"_dev.conll\"\n        test_path = PATH +\"/\"+language+\"/\"+locale+\"_test.conll\"\n#         train_path = PATH +\"/\"+language+\"/\"+locale+\"_train.conll\"\n#         print(dev_path,os.path.isfile(dev_path))\n#         print(test_path,os.path.isfile(test_path))\n        print(train_path,os.path.isfile(train_path))\n        tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n        config = RobertaConfig.from_pretrained(\"FacebookAI/roberta-base\")\n        num_labels = len(label_map)\n        model = NERModel(num_labels, config.hidden_size)\n        freeze_weignts(model)\n        print(num_labels,config.hidden_size)\n#         train_dataset = MultiCoNERDataset(train_path, tokenizer)  # Create train dataset for language\n#         val_dataset = MultiCoNERDataset(dev_path,tokenizer, regime=\"validation\")  # Create validation dataset for language\n        test_dataset = MultiCoNERDataset(test_path, tokenizer, regime=\"test\")  # Create test dataset for language\n#         train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,collate_fn=collate_fn)  # Train DataLoader\n#         val_loader = DataLoader(val_dataset, batch_size=32,collate_fn=collate_fn)  # Validation DataLoader\n        test_loader = DataLoader(test_dataset, batch_size=32,collate_fn=collate_fn)  # Test DataLoader\n#         train_and_checkpoint(model, locale, train_loader, val_loader)  # Train and checkpoint\n        print(f'Test for {local} language')\n        test(locale, test_loader)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ja7dLPqthWUj","outputId":"faf4e176-3505-4172-bfa4-9fb7afbb3104","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\n# from transformers import XLMRobertaTokenizer, XLMRobertaConfig\n# file_path = \"/kaggle/input/multiconer/multiconer2023/BN-Bangla/bn_train.conll\"  # Path to your dataset file\n# tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")  # Use XLM-RoBERTa tokenizer\n# dataset = MultiCoNERDataset(file_path, tokenizer)\n\n# # Example of accessing a sample\n# sample_idx = 0\n# input_ids, attention_mask, labels = dataset[sample_idx]\n# print(\"Input IDs:\", input_ids.size())\n# print(\"Attention Mask:\", attention_mask.size())\n# print(\"Labels:\", labels.size())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:22:39.645606Z","iopub.execute_input":"2024-04-25T22:22:39.646198Z","iopub.status.idle":"2024-04-25T22:22:43.640226Z","shell.execute_reply.started":"2024-04-25T22:22:39.646150Z","shell.execute_reply":"2024-04-25T22:22:43.638033Z"},"id":"QKSXiISyhWUk","outputId":"e5582b09-7e7e-42ca-e5ed-39153807a9ab","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Tokens before padding: ['স্টেশনটি', 'প্ল্যাটফর্ম', 'স্ক্রিন', 'ডোর', 'দিয়ে', 'সজ্জিত।']\nLabels before padding: ['O', 'B-OtherPROD', 'I-OtherPROD', 'I-OtherPROD', 'O', 'O']\nInput IDs: torch.Size([512])\nAttention Mask: torch.Size([512])\nLabels: torch.Size([6])\n","output_type":"stream"}]},{"cell_type":"code","source":"# def tokenize_and_align_labels(examples, tokenizer, label_all_tokens=True):\n#     tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n#     labels = []\n#     for i, label in enumerate(examples[\"ner_tags\"]):\n#         word_ids = tokenized_inputs.word_ids(batch_index=i)\n#         previous_word_idx = None\n#         label_ids = []\n#         for word_idx in word_ids:\n#             if word_idx is None:\n#                 label_ids.append(-100)\n#             elif word_idx != previous_word_idx:\n#                 label_ids.append(label[word_idx])\n#             else:\n#                 label_ids.append(label[word_idx] if label_all_tokens else -100)\n#             previous_word_idx = word_idx\n#         labels.append(label_ids)\n#     tokenized_inputs[\"labels\"] = labels\n#     return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-04-25T16:19:36.070944Z","iopub.execute_input":"2024-04-25T16:19:36.071932Z","iopub.status.idle":"2024-04-25T16:19:36.079055Z","shell.execute_reply.started":"2024-04-25T16:19:36.071897Z","shell.execute_reply":"2024-04-25T16:19:36.078145Z"},"id":"V57RJYZYhWUm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# d={}\n# d[\"tokens\"] = ['karla ', 'cossío ', 'as ', 'pilar ', 'gandía ', '( ', 'recurring ', 'season', '1', ';', 'guest', '2', ')']\n# d[\"ner_tags\"] = ['B-Artist', 'I-Artist', 'O', 'B-VisualWork', 'I-VisualWork', 'O', 'O', 'O', 'O', 'O','O','O','O']","metadata":{"execution":{"iopub.status.busy":"2024-04-25T16:15:18.621184Z","iopub.execute_input":"2024-04-25T16:15:18.621929Z","iopub.status.idle":"2024-04-25T16:15:18.627378Z","shell.execute_reply.started":"2024-04-25T16:15:18.621887Z","shell.execute_reply":"2024-04-25T16:15:18.626436Z"},"id":"6mg6AmkYhWUn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n\n# xlmr_model_name = \"xlm-roberta-base\"\n# xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n\n# text = \"Jack Sparrow loves New York!\"\n# text2=text.split(\" \")\n# #xlmr_tokens = xlmr_tokenizer(text).tokens()\n# a=tokenize_and_align_labels(d, xlmr_tokenizer)\n# print(a)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T16:20:42.417310Z","iopub.execute_input":"2024-04-25T16:20:42.418012Z","iopub.status.idle":"2024-04-25T16:20:44.498555Z","shell.execute_reply.started":"2024-04-25T16:20:42.417980Z","shell.execute_reply":"2024-04-25T16:20:44.497285Z"},"id":"UmjyOwukhWUo","outputId":"47724b56-d871-4b03-87a6-86efce7ec77a","trusted":true},"execution_count":null,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[83], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m text2\u001b[38;5;241m=\u001b[39mtext\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#xlmr_tokens = xlmr_tokenizer(text).tokens()\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxlmr_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n","Cell \u001b[0;32mIn[81], line 5\u001b[0m, in \u001b[0;36mtokenize_and_align_labels\u001b[0;34m(examples, label_all_tokens)\u001b[0m\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m----> 5\u001b[0m     word_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     previous_word_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     label_ids \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:381\u001b[0m, in \u001b[0;36mBatchEncoding.word_ids\u001b[0;34m(self, batch_index)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03mReturn a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    (several tokens will be mapped to the same word index if they are parts of that word).\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings:\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m     )\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[batch_index]\u001b[38;5;241m.\u001b[39mword_ids\n","\u001b[0;31mValueError\u001b[0m: word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class)."],"ename":"ValueError","evalue":"word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class).","output_type":"error"}]},{"cell_type":"code","source":"# !pip install tner","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"egMVaZUn3pK8","outputId":"d8090a24-60ca-48cc-d589-c61295cade22"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":"Collecting tner\n\n  Using cached tner-0.2.4.tar.gz (2.2 MB)\n\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from tner) (2.2.1+cu121)\n\nCollecting allennlp>=2.0.0 (from tner)\n\n  Using cached allennlp-2.10.1-py3-none-any.whl (730 kB)\n\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from tner) (4.40.0)\n\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tner) (0.1.99)\n\nCollecting seqeval (from tner)\n\n  Using cached seqeval-1.2.2.tar.gz (43 kB)\n\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\nCollecting datasets (from tner)\n\n  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting torch (from tner)\n\n  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/torchvision/\u001b[0m\u001b[33m\n\n\u001b[0mCollecting torchvision<0.14.0,>=0.8.1 (from allennlp>=2.0.0->tner)\n\n  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3 (from allennlp>=2.0.0->tner)\n\n  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n\nCollecting fairscale==0.4.6 (from allennlp>=2.0.0->tner)\n\n  Downloading fairscale-0.4.6.tar.gz (248 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\nRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (3.8.1)\n\nCollecting spacy<3.4,>=2.1.0 (from allennlp>=2.0.0->tner)\n\n  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (1.25.2)\n\nCollecting tensorboardX>=1.2 (from allennlp>=2.0.0->tner)\n\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (2.31.0)\n\nRequirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (4.66.2)\n\nRequirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (3.9.0)\n\nRequirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (1.2.2)\n\nRequirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (1.11.4)\n\nRequirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (7.4.4)\n\nCollecting transformers (from tner)\n\n  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting filelock<3.8,>=3.3 (from allennlp>=2.0.0->tner)\n\n  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n\nCollecting lmdb>=1.2.1 (from allennlp>=2.0.0->tner)\n\n  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (10.1.0)\n\nCollecting termcolor==1.1.0 (from allennlp>=2.0.0->tner)\n\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\nCollecting wandb<0.13.0,>=0.10.0 (from allennlp>=2.0.0->tner)\n\n  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (0.20.3)\n\nCollecting dill>=0.3.4 (from allennlp>=2.0.0->tner)\n\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting base58>=2.1.1 (from allennlp>=2.0.0->tner)\n\n  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n\nCollecting sacremoses (from allennlp>=2.0.0->tner)\n\n  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (0.9.4)\n\nRequirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (3.20.3)\n\nRequirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner) (5.7.1)\n\nCollecting jsonnet>=0.10.0 (from allennlp>=2.0.0->tner)\n\n  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->tner) (4.11.0)\n\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->tner) (24.0)\n\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->tner) (6.0.1)\n\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->tner) (2023.12.25)\n\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers->tner)\n\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->tner) (14.0.2)\n\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->tner) (0.6)\n\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->tner) (2.0.3)\n\nCollecting xxhash (from datasets->tner)\n\n  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting multiprocess (from datasets->tner)\n\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->tner) (2023.6.0)\n\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->tner) (3.9.5)\n\nCollecting huggingface-hub>=0.0.16 (from allennlp>=2.0.0->tner)\n\n  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting rich<13.0,>=12.1 (from cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner)\n\n  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting boto3<2.0,>=1.0 (from cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner)\n\n  Downloading boto3-1.34.92-py3-none-any.whl (139 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (2.8.0)\n\nINFO: pip is looking at multiple versions of cached-path to determine which version is compatible with other requirements. This could take a while.\n\nCollecting cached-path<1.2.0,>=1.1.3 (from allennlp>=2.0.0->tner)\n\n  Downloading cached_path-1.1.5-py3-none-any.whl (26 kB)\n\n  Downloading cached_path-1.1.4-py3-none-any.whl (26 kB)\n\n  Downloading cached_path-1.1.3-py3-none-any.whl (26 kB)\n\nCollecting datasets (from tner)\n\n  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hINFO: pip is looking at multiple versions of cached-path to determine which version is compatible with other requirements. This could take a while.\n\n  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n\n  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting dill>=0.3.4 (from allennlp>=2.0.0->tner)\n\n  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting datasets (from tner)\n\n  Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.14.2-py3-none-any.whl (518 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.14.1-py3-none-any.whl (492 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.4/492.4 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.14.0-py3-none-any.whl (492 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.13.2-py3-none-any.whl (512 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.7/512.7 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting dill>=0.3.4 (from allennlp>=2.0.0->tner)\n\n  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting datasets (from tner)\n\n  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting responses<0.19 (from datasets->tner)\n\n  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n\nCollecting datasets (from tner)\n\n  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting huggingface-hub>=0.0.16 (from allennlp>=2.0.0->tner)\n\n  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tner) (1.3.1)\n\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tner) (23.2.0)\n\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tner) (1.4.1)\n\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tner) (6.0.5)\n\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tner) (1.9.4)\n\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tner) (4.0.3)\n\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp>=2.0.0->tner) (8.1.7)\n\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp>=2.0.0->tner) (1.4.0)\n\nRequirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp>=2.0.0->tner) (2.0.0)\n\nRequirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp>=2.0.0->tner) (1.4.0)\n\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp>=2.0.0->tner) (1.2.1)\n\nRequirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp>=2.0.0->tner) (2.0.1)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp>=2.0.0->tner) (3.3.2)\n\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp>=2.0.0->tner) (3.7)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp>=2.0.0->tner) (2.0.7)\n\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp>=2.0.0->tner) (2024.2.2)\n\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->allennlp>=2.0.0->tner) (3.4.0)\n\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (3.0.12)\n\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (1.0.5)\n\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (1.0.10)\n\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (2.0.8)\n\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (3.0.9)\n\nCollecting thinc<8.1.0,>=8.0.14 (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner)\n\n  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (0.7.11)\n\nCollecting wasabi<1.1.0,>=0.9.1 (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner)\n\n  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (2.4.8)\n\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (2.0.10)\n\nCollecting typer>=0.4.1 (from allennlp>=2.0.0->tner)\n\n  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n\nCollecting pathy>=0.3.5 (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner)\n\n  Downloading pathy-0.11.0-py3-none-any.whl (47 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (6.4.0)\n\nCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner)\n\n  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (3.1.3)\n\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (67.7.2)\n\nCollecting typing-extensions (from torch->tner)\n\n  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (3.3.0)\n\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp>=2.0.0->tner) (9.4.0)\n\nCollecting GitPython>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner)\n\n  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner) (2.3)\n\nCollecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner)\n\n  Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner) (5.9.5)\n\nCollecting sentry-sdk>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner)\n\n  Downloading sentry_sdk-2.0.0-py2.py3-none-any.whl (266 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner) (1.16.0)\n\nCollecting docker-pycreds>=0.4.0 (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner)\n\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n\nCollecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner)\n\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\n\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\nCollecting setproctitle (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner)\n\n  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n\nCollecting multiprocess (from datasets->tner)\n\n  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25h  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->tner) (2.8.2)\n\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->tner) (2023.4)\n\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->tner) (2024.1)\n\nCollecting botocore<1.35.0,>=1.34.92 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner)\n\n  Downloading botocore-1.34.92-py3-none-any.whl (12.2 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner)\n\n  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n\nCollecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner)\n\n  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner)\n\n  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (2.27.0)\n\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (2.11.1)\n\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (2.3.3)\n\nRequirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (2.7.0)\n\nCollecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner)\n\n  Downloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n\nCollecting commonmark<0.10.0,>=0.9.0 (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner)\n\n  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (2.16.1)\n\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp>=2.0.0->tner) (2.1.5)\n\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner)\n\n  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (1.63.0)\n\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (5.3.3)\n\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (0.4.0)\n\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (4.9)\n\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (1.5.0)\n\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner) (0.6.0)\n\nBuilding wheels for collected packages: tner, fairscale, termcolor, seqeval, jsonnet, pathtools\n\n  Building wheel for tner (setup.py) ... \u001b[?25l\u001b[?25hdone\n\n  Created wheel for tner: filename=tner-0.2.4-py3-none-any.whl size=39758 sha256=f4fce12e8c9c90ff5f30d75d11001f6df07eb5c646810a9565f395136e86a61d\n\n  Stored in directory: /root/.cache/pip/wheels/06/57/f6/297e90e242ab8befffe9b1fe05896097c67ba2322a0da606a4\n\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\n  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307222 sha256=6fb67d455c66ce7c34260d7aaabdf44d8d9a27c2e1cb809a23552092a96ff5fa\n\n  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n\n  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=1177a8d51d0b585ffce21a550ec07d1372cb8e12986d10bf25a0d1e0965b899a\n\n  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=f476ab64974e47c2e764da84d2822ac9ce8df6e303a548e129ed9066983cb193\n\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n\n  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n\n  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6406880 sha256=2ac65d095baae499f164b5ba1160d8d29b2c9057d1756f1ca0a8217a7f5e3101\n\n  Stored in directory: /root/.cache/pip/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\n\n  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n\n  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=f0569896d9dd8787b6300ed971ff2a813ee5cbf9a06a19c8153a4eeccfa678f4\n\n  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n\nSuccessfully built tner fairscale termcolor seqeval jsonnet pathtools\n\nInstalling collected packages: wasabi, tokenizers, termcolor, pathtools, lmdb, jsonnet, commonmark, xxhash, typing-extensions, typer, tensorboardX, smmap, shortuuid, setproctitle, sentry-sdk, sacremoses, rich, pathlib-abc, jmespath, filelock, docker-pycreds, dill, base58, torch, responses, pydantic, pathy, multiprocess, huggingface-hub, gitdb, botocore, transformers, torchvision, thinc, seqeval, s3transfer, GitPython, fairscale, wandb, spacy, datasets, boto3, cached-path, allennlp, tner\n\n  Attempting uninstall: wasabi\n\n    Found existing installation: wasabi 1.1.2\n\n    Uninstalling wasabi-1.1.2:\n\n      Successfully uninstalled wasabi-1.1.2\n\n  Attempting uninstall: tokenizers\n\n    Found existing installation: tokenizers 0.19.1\n\n    Uninstalling tokenizers-0.19.1:\n\n      Successfully uninstalled tokenizers-0.19.1\n\n  Attempting uninstall: termcolor\n\n    Found existing installation: termcolor 2.4.0\n\n    Uninstalling termcolor-2.4.0:\n\n      Successfully uninstalled termcolor-2.4.0\n\n  Attempting uninstall: typing-extensions\n\n    Found existing installation: typing_extensions 4.11.0\n\n    Uninstalling typing_extensions-4.11.0:\n\n      Successfully uninstalled typing_extensions-4.11.0\n\n  Attempting uninstall: typer\n\n    Found existing installation: typer 0.9.4\n\n    Uninstalling typer-0.9.4:\n\n      Successfully uninstalled typer-0.9.4\n\n  Attempting uninstall: rich\n\n    Found existing installation: rich 13.7.1\n\n    Uninstalling rich-13.7.1:\n\n      Successfully uninstalled rich-13.7.1\n\n  Attempting uninstall: filelock\n\n    Found existing installation: filelock 3.13.4\n\n    Uninstalling filelock-3.13.4:\n\n      Successfully uninstalled filelock-3.13.4\n\n  Attempting uninstall: torch\n\n    Found existing installation: torch 2.2.1+cu121\n\n    Uninstalling torch-2.2.1+cu121:\n\n      Successfully uninstalled torch-2.2.1+cu121\n\n  Attempting uninstall: pydantic\n\n    Found existing installation: pydantic 2.7.0\n\n    Uninstalling pydantic-2.7.0:\n\n      Successfully uninstalled pydantic-2.7.0\n\n  Attempting uninstall: huggingface-hub\n\n    Found existing installation: huggingface-hub 0.20.3\n\n    Uninstalling huggingface-hub-0.20.3:\n\n      Successfully uninstalled huggingface-hub-0.20.3\n\n  Attempting uninstall: transformers\n\n    Found existing installation: transformers 4.40.0\n\n    Uninstalling transformers-4.40.0:\n\n      Successfully uninstalled transformers-4.40.0\n\n  Attempting uninstall: torchvision\n\n    Found existing installation: torchvision 0.17.1+cu121\n\n    Uninstalling torchvision-0.17.1+cu121:\n\n      Successfully uninstalled torchvision-0.17.1+cu121\n\n  Attempting uninstall: thinc\n\n    Found existing installation: thinc 8.2.3\n\n    Uninstalling thinc-8.2.3:\n\n      Successfully uninstalled thinc-8.2.3\n\n  Attempting uninstall: spacy\n\n    Found existing installation: spacy 3.7.4\n\n    Uninstalling spacy-3.7.4:\n\n      Successfully uninstalled spacy-3.7.4\n\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n\nsqlalchemy 2.0.29 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n\nen-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.3.3 which is incompatible.\n\ninflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n\npydantic-core 2.18.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n\ntorchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.12.1 which is incompatible.\n\ntorchdata 0.7.1 requires torch>=2, but you have torch 1.12.1 which is incompatible.\n\ntorchtext 0.17.1 requires torch==2.2.1, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n\n\u001b[0mSuccessfully installed GitPython-3.1.43 allennlp-2.10.1 base58-2.1.1 boto3-1.34.92 botocore-1.34.92 cached-path-1.1.6 commonmark-0.9.1 datasets-2.10.1 dill-0.3.6 docker-pycreds-0.4.0 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.11 huggingface-hub-0.10.1 jmespath-1.0.1 jsonnet-0.20.0 lmdb-1.4.1 multiprocess-0.70.14 pathlib-abc-0.1.1 pathtools-0.1.2 pathy-0.11.0 pydantic-1.8.2 responses-0.18.0 rich-12.6.0 s3transfer-0.10.1 sacremoses-0.1.1 sentry-sdk-2.0.0 seqeval-1.2.2 setproctitle-1.3.3 shortuuid-1.0.13 smmap-5.0.1 spacy-3.3.3 tensorboardX-2.6.2.2 termcolor-1.1.0 thinc-8.0.17 tner-0.2.4 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 typing-extensions-4.5.0 wandb-0.12.21 wasabi-0.10.1 xxhash-3.4.1\n"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["filelock","huggingface_hub","pydantic","tokenizers","torch","torchgen","transformers"]},"id":"f1d0f8114eb24e7ca5f1c05cb51fee93"}},"metadata":{}}]}]}