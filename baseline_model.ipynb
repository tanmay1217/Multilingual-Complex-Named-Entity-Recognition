{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7898378,"sourceType":"datasetVersion","datasetId":4638336},{"sourceId":8208563,"sourceType":"datasetVersion","datasetId":4864187}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!while IFS= read -r line; do pip install \"$line\"; done < '/kaggle/input/requirement/requirement.txt'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-lightning","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:36:32.668670Z","iopub.execute_input":"2024-04-25T12:36:32.669120Z","iopub.status.idle":"2024-04-25T12:39:08.248386Z","shell.execute_reply.started":"2024-04-25T12:36:32.669070Z","shell.execute_reply":"2024-04-25T12:39:08.246926Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.26.4)\nCollecting torch>=1.13.0 (from pytorch-lightning)\n  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.66.1)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (6.0.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.3.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (21.3)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.5.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (0.11.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch-lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.7.1)\nCollecting typing-extensions>=4.4.0 (from pytorch-lightning)\n  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.0 (from torch>=1.13.0->pytorch-lightning)\n  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->pytorch-lightning)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\nDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\nDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: typing-extensions, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\n  Attempting uninstall: torch\n    Found existing installation: torch 1.12.1\n    Uninstalling torch-1.12.1:\n      Successfully uninstalled torch-1.12.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nallennlp 2.10.1 requires torch<1.13.0,>=1.10.0, but you have torch 2.3.0 which is incompatible.\nallennlp 2.10.1 requires transformers<4.21,>=4.1, but you have transformers 4.40.1 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\nfastai 2.7.14 requires torch<2.3,>=1.10, but you have torch 2.3.0 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nspacy 3.3.3 requires typing-extensions<4.6.0,>=3.7.4.1, but you have typing-extensions 4.11.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\ntorchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 2.3.0 which is incompatible.\ntorchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 2.3.0 which is incompatible.\ntorchvision 0.13.1 requires torch==1.12.1, but you have torch 2.3.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires pydantic>=2, but you have pydantic 1.8.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 triton-2.3.0 typing-extensions-4.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install allennlp\n!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:31:15.956746Z","iopub.execute_input":"2024-04-25T12:31:15.957161Z","iopub.status.idle":"2024-04-25T12:36:32.665084Z","shell.execute_reply.started":"2024-04-25T12:31:15.957126Z","shell.execute_reply":"2024-04-25T12:36:32.663102Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting allennlp\n  Downloading allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)\nCollecting torch<1.13.0,>=1.10.0 (from allennlp)\n  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\nCollecting torchvision<0.14.0,>=0.8.1 (from allennlp)\n  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\nCollecting cached-path<1.2.0,>=1.1.3 (from allennlp)\n  Downloading cached_path-1.1.6-py3-none-any.whl.metadata (6.0 kB)\nCollecting fairscale==0.4.6 (from allennlp)\n  Downloading fairscale-0.4.6.tar.gz (248 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting nltk>=3.6.5 (from allennlp)\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nCollecting spacy<3.4,>=2.1.0 (from allennlp)\n  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\nRequirement already satisfied: numpy>=1.21.4 in /opt/conda/lib/python3.10/site-packages (from allennlp) (1.26.4)\nRequirement already satisfied: tensorboardX>=1.2 in /opt/conda/lib/python3.10/site-packages (from allennlp) (2.6.2.2)\nRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.10/site-packages (from allennlp) (2.31.0)\nRequirement already satisfied: tqdm>=4.62 in /opt/conda/lib/python3.10/site-packages (from allennlp) (4.66.1)\nRequirement already satisfied: h5py>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from allennlp) (3.10.0)\nRequirement already satisfied: scikit-learn>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from allennlp) (1.2.2)\nRequirement already satisfied: scipy>=1.7.3 in /opt/conda/lib/python3.10/site-packages (from allennlp) (1.11.4)\nRequirement already satisfied: pytest>=6.2.5 in /opt/conda/lib/python3.10/site-packages (from allennlp) (8.1.1)\nCollecting transformers<4.21,>=4.1 (from allennlp)\n  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.96 in /opt/conda/lib/python3.10/site-packages (from allennlp) (0.2.0)\nCollecting filelock<3.8,>=3.3 (from allennlp)\n  Downloading filelock-3.7.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting lmdb>=1.2.1 (from allennlp)\n  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\nRequirement already satisfied: more-itertools>=8.12.0 in /opt/conda/lib/python3.10/site-packages (from allennlp) (10.2.0)\nCollecting termcolor==1.1.0 (from allennlp)\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting wandb<0.13.0,>=0.10.0 (from allennlp)\n  Downloading wandb-0.12.21-py2.py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: huggingface-hub>=0.0.16 in /opt/conda/lib/python3.10/site-packages (from allennlp) (0.22.2)\nRequirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from allennlp) (0.3.8)\nCollecting base58>=2.1.1 (from allennlp)\n  Downloading base58-2.1.1-py3-none-any.whl.metadata (3.1 kB)\nCollecting sacremoses (from allennlp)\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: typer>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from allennlp) (0.9.0)\nRequirement already satisfied: protobuf<4.0.0,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from allennlp) (3.20.3)\nRequirement already satisfied: traitlets>5.1.1 in /opt/conda/lib/python3.10/site-packages (from allennlp) (5.9.0)\nCollecting jsonnet>=0.10.0 (from allennlp)\n  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting rich<13.0,>=12.1 (from cached-path<1.2.0,>=1.1.3->allennlp)\n  Downloading rich-12.6.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: boto3<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.26.100)\nRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.44.0)\nCollecting huggingface-hub>=0.0.16 (from allennlp)\n  Downloading huggingface_hub-0.10.1-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.16->allennlp) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.16->allennlp) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.16->allennlp) (21.3)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.6.5->allennlp) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>=3.6.5->allennlp) (1.4.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.6.5->allennlp) (2023.12.25)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest>=6.2.5->allennlp) (2.0.0)\nRequirement already satisfied: pluggy<2.0,>=1.4 in /opt/conda/lib/python3.10/site-packages (from pytest>=6.2.5->allennlp) (1.4.0)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest>=6.2.5->allennlp) (1.2.0)\nRequirement already satisfied: tomli>=1 in /opt/conda/lib/python3.10/site-packages (from pytest>=6.2.5->allennlp) (2.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->allennlp) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->allennlp) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->allennlp) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->allennlp) (2024.2.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.1->allennlp) (3.2.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.9)\nCollecting thinc<8.1.0,>=8.0.14 (from spacy<3.4,>=2.1.0->allennlp)\n  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.11)\nCollecting wasabi<1.1.0,>=0.9.1 (from spacy<3.4,>=2.1.0->allennlp)\n  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.10)\nCollecting typer>=0.4.1 (from allennlp)\n  Downloading typer-0.4.2-py3-none-any.whl.metadata (12 kB)\nCollecting pathy>=0.3.5 (from spacy<3.4,>=2.1.0->allennlp)\n  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (6.4.0)\nCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.4,>=2.1.0->allennlp)\n  Downloading pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (69.0.3)\nCollecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.0.16->allennlp)\n  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.5.0)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<4.21,>=4.1->allennlp)\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.41)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\nCollecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.45.0)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.16.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.4.0)\nCollecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp)\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.3.3)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.11)\nRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.26.1)\nRequirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.11.1)\nRequirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.4.1)\nRequirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.7.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.0.16->allennlp) (3.1.1)\nCollecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp)\n  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\nCollecting commonmark<0.10.0,>=0.9.0 (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp)\n  Downloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.3)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.9.0.post0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (5.0.1)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.62.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.5.1)\nDownloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading base58-2.1.1-py3-none-any.whl (5.6 kB)\nDownloading cached_path-1.1.6-py3-none-any.whl (26 kB)\nDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\nDownloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading typer-0.4.2-py3-none-any.whl (27 kB)\nDownloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m463.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\nDownloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading rich-12.6.0-py3-none-any.whl (237 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\nDownloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\nDownloading wasabi-0.10.1-py3-none-any.whl (26 kB)\nDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: fairscale, termcolor, jsonnet, pathtools\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307222 sha256=dea77c0d97b42a866fb26b8a81e53b64828c1959bacddd68243e23fb61967b95\n  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=da0ebeccd55b9425d4edf150336ce99c1c5f3b8613712c19086e7d5846105d52\n  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6605204 sha256=40291102dc475e901fce1ef3ae01b780d2da37549458e14e7ee3c82d29fcb1c3\n  Stored in directory: /root/.cache/pip/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\n  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=e521f03467738e5a3e1fac638033ed4d3b8ae04ab76d5dcb4599e5bb9ab11f5d\n  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\nSuccessfully built fairscale termcolor jsonnet pathtools\nInstalling collected packages: wasabi, tokenizers, termcolor, pathtools, lmdb, jsonnet, commonmark, typing-extensions, typer, shortuuid, sacremoses, rich, pathlib-abc, nltk, filelock, base58, torch, pydantic, pathy, huggingface-hub, botocore, wandb, transformers, torchvision, thinc, fairscale, spacy, cached-path, allennlp\n  Attempting uninstall: wasabi\n    Found existing installation: wasabi 1.1.2\n    Uninstalling wasabi-1.1.2:\n      Successfully uninstalled wasabi-1.1.2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: termcolor\n    Found existing installation: termcolor 2.4.0\n    Uninstalling termcolor-2.4.0:\n      Successfully uninstalled termcolor-2.4.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.9.0\n    Uninstalling typing_extensions-4.9.0:\n      Successfully uninstalled typing_extensions-4.9.0\n  Attempting uninstall: typer\n    Found existing installation: typer 0.9.0\n    Uninstalling typer-0.9.0:\n      Successfully uninstalled typer-0.9.0\n  Attempting uninstall: rich\n    Found existing installation: rich 13.7.0\n    Uninstalling rich-13.7.0:\n      Successfully uninstalled rich-13.7.0\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.13.1\n    Uninstalling filelock-3.13.1:\n      Successfully uninstalled filelock-3.13.1\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2+cpu\n    Uninstalling torch-2.1.2+cpu:\n      Successfully uninstalled torch-2.1.2+cpu\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.5.3\n    Uninstalling pydantic-2.5.3:\n      Successfully uninstalled pydantic-2.5.3\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.22.2\n    Uninstalling huggingface-hub-0.22.2:\n      Successfully uninstalled huggingface-hub-0.22.2\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.34.69\n    Uninstalling botocore-1.34.69:\n      Successfully uninstalled botocore-1.34.69\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.16.6\n    Uninstalling wandb-0.16.6:\n      Successfully uninstalled wandb-0.16.6\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.16.2+cpu\n    Uninstalling torchvision-0.16.2+cpu:\n      Successfully uninstalled torchvision-0.16.2+cpu\n  Attempting uninstall: thinc\n    Found existing installation: thinc 8.2.3\n    Uninstalling thinc-8.2.3:\n      Successfully uninstalled thinc-8.2.3\n  Attempting uninstall: spacy\n    Found existing installation: spacy 3.7.4\n    Uninstalling spacy-3.7.4:\n      Successfully uninstalled spacy-3.7.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nsqlalchemy 2.0.25 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\naiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.29.165 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ndatasets 2.18.0 requires huggingface-hub>=0.19.4, but you have huggingface-hub 0.10.1 which is incompatible.\nen-core-web-lg 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.3.3 which is incompatible.\nen-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.3.3 which is incompatible.\nfastapi 0.108.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.20.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\npydantic-core 2.14.6 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\npytorch-lightning 2.2.2 requires torch>=1.13.0, but you have torch 1.12.1 which is incompatible.\nstable-baselines3 2.1.0 requires torch>=1.13, but you have torch 1.12.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\ntorchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 1.12.1 which is incompatible.\ntorchdata 0.7.1 requires torch>=2, but you have torch 1.12.1 which is incompatible.\ntorchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 1.12.1 which is incompatible.\ntypeguard 4.1.5 requires typing-extensions>=4.7.0; python_version < \"3.12\", but you have typing-extensions 4.5.0 which is incompatible.\nvirtualenv 20.21.0 requires platformdirs<4,>=2.4, but you have platformdirs 4.2.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires pydantic>=2, but you have pydantic 1.8.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed allennlp-2.10.1 base58-2.1.1 botocore-1.29.165 cached-path-1.1.6 commonmark-0.9.1 fairscale-0.4.6 filelock-3.7.1 huggingface-hub-0.10.1 jsonnet-0.20.0 lmdb-1.4.1 nltk-3.8.1 pathlib-abc-0.1.1 pathtools-0.1.2 pathy-0.11.0 pydantic-1.8.2 rich-12.6.0 sacremoses-0.1.1 shortuuid-1.0.13 spacy-3.3.3 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 typing-extensions-4.5.0 wandb-0.12.21 wasabi-0.10.1\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.20.1)\nCollecting transformers\n  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.7.1)\nCollecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nCollecting tokenizers<0.20,>=0.19 (from transformers)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.10.1\n    Uninstalling huggingface-hub-0.10.1:\n      Successfully uninstalled huggingface-hub-0.10.1\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Uninstalling tokenizers-0.12.1:\n      Successfully uninstalled tokenizers-0.12.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.20.1\n    Uninstalling transformers-4.20.1:\n      Successfully uninstalled transformers-4.20.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.10.1 requires transformers<4.21,>=4.1, but you have transformers 4.40.1 which is incompatible.\ncached-path 1.1.6 requires huggingface-hub<0.11.0,>=0.8.1, but you have huggingface-hub 0.22.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.22.2 tokenizers-0.19.1 transformers-4.40.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\ndef setup_custom_logger(name, level='INFO'):\n    formatter = logging.Formatter(fmt='%(asctime)s - %(levelname)s - %(module)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    if len(logger.handlers) == 0:\n        logger.addHandler(handler)\n    return logger\n\nlogger = setup_custom_logger('root')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:39:32.765969Z","iopub.execute_input":"2024-04-25T12:39:32.766453Z","iopub.status.idle":"2024-04-25T12:39:32.776523Z","shell.execute_reply.started":"2024-04-25T12:39:32.766407Z","shell.execute_reply":"2024-04-25T12:39:32.775510Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning import Trainer","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:39:34.708495Z","iopub.execute_input":"2024-04-25T12:39:34.708937Z","iopub.status.idle":"2024-04-25T12:39:43.447222Z","shell.execute_reply.started":"2024-04-25T12:39:34.708904Z","shell.execute_reply":"2024-04-25T12:39:43.445735Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n  warn(f\"Failed to load image Python extension: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom collections import defaultdict\nfrom typing import Set\nfrom overrides import overrides\n\nfrom allennlp.training.metrics.metric import Metric\n\n\nclass SpanF1(Metric):\n    def __init__(self, non_entity_labels=['O']) -> None:\n        self._num_gold_mentions = 0\n        self._num_recalled_mentions = 0\n        self._num_predicted_mentions = 0\n        self._TP, self._FP, self._GT = defaultdict(int), defaultdict(int), defaultdict(int)\n        self.non_entity_labels = set(non_entity_labels)\n\n    @overrides\n    def __call__(self, batched_predicted_spans, batched_gold_spans, sentences=None):\n        non_entity_labels = self.non_entity_labels\n\n        for predicted_spans, gold_spans in zip(batched_predicted_spans, batched_gold_spans):\n            gold_spans_set = set([x for x, y in gold_spans.items() if y not in non_entity_labels])\n            pred_spans_set = set([x for x, y in predicted_spans.items() if y not in non_entity_labels])\n\n            self._num_gold_mentions += len(gold_spans_set)\n            self._num_recalled_mentions += len(gold_spans_set & pred_spans_set)\n            self._num_predicted_mentions += len(pred_spans_set)\n\n            for ky, val in gold_spans.items():\n                if val not in non_entity_labels:\n                    self._GT[val] += 1\n\n            for ky, val in predicted_spans.items():\n                if val in non_entity_labels:\n                    continue\n                if ky in gold_spans and val == gold_spans[ky]:\n                    self._TP[val] += 1\n                else:\n                    self._FP[val] += 1\n\n    @overrides\n    def get_metric(self, reset: bool = False) -> float:\n        all_tags: Set[str] = set()\n        all_tags.update(self._TP.keys())\n        all_tags.update(self._FP.keys())\n        all_tags.update(self._GT.keys())\n        all_metrics = {}\n\n        for tag in all_tags:\n            precision, recall, f1_measure = self.compute_prf_metrics(true_positives=self._TP[tag],\n                                                                     false_negatives=self._GT[tag] - self._TP[tag],\n                                                                     false_positives=self._FP[tag])\n            all_metrics['P@{}'.format(tag)] = precision\n            all_metrics['R@{}'.format(tag)] = recall\n            all_metrics['F1@{}'.format(tag)] = f1_measure\n\n        # Compute the precision, recall and f1 for all spans jointly.\n        precision, recall, f1_measure = self.compute_prf_metrics(true_positives=sum(self._TP.values()),\n                                                                 false_positives=sum(self._FP.values()),\n                                                                 false_negatives=sum(self._GT.values())-sum(self._TP.values()))\n        all_metrics[\"micro@P\"] = precision\n        all_metrics[\"micro@R\"] = recall\n        all_metrics[\"micro@F1\"] = f1_measure\n\n        if self._num_gold_mentions == 0:\n            entity_recall = 0.0\n        else:\n            entity_recall = self._num_recalled_mentions / float(self._num_gold_mentions)\n\n        if self._num_predicted_mentions == 0:\n            entity_precision = 0.0\n        else:\n            entity_precision = self._num_recalled_mentions / float(self._num_predicted_mentions)\n\n        all_metrics['MD@R'] = entity_recall\n        all_metrics['MD@P'] = entity_precision\n        all_metrics['MD@F1'] = 2. * ((entity_precision * entity_recall) / (entity_precision + entity_recall + 1e-13))\n        all_metrics['ALLTRUE'] = self._num_gold_mentions\n        all_metrics['ALLRECALLED'] = self._num_recalled_mentions\n        all_metrics['ALLPRED'] = self._num_predicted_mentions\n        if reset:\n            self.reset()\n        return all_metrics\n\n    @staticmethod\n    def compute_prf_metrics(true_positives: int, false_positives: int, false_negatives: int):\n        precision = float(true_positives) / float(true_positives + false_positives + 1e-13)\n        recall = float(true_positives) / float(true_positives + false_negatives + 1e-13)\n        f1_measure = 2. * ((precision * recall) / (precision + recall + 1e-13))\n        return precision, recall, f1_measure\n\n    @overrides\n    def reset(self):\n        self._num_gold_mentions = 0\n        self._num_recalled_mentions = 0\n        self._num_predicted_mentions = 0\n        self._TP.clear()\n        self._FP.clear()\n        self._GT.clear()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:39:45.319601Z","iopub.execute_input":"2024-04-25T12:39:45.320247Z","iopub.status.idle":"2024-04-25T12:40:58.927834Z","shell.execute_reply.started":"2024-04-25T12:39:45.320210Z","shell.execute_reply":"2024-04-25T12:40:58.926320Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-04-25 12:39:47.885160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-25 12:39:47.885358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-25 12:39:48.058382: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nfrom transformers import AutoTokenizer\n\n\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\nclass CoNLLReader(Dataset):\n    def __init__(self, max_instances=-1, max_length=50, target_vocab=None, pretrained_dir='', encoder_model='xlm-roberta-large'):\n        self._max_instances = max_instances\n        self._max_length = max_length\n\n        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_dir + encoder_model)\n\n        self.pad_token = self.tokenizer.special_tokens_map['pad_token']\n        self.pad_token_id = self.tokenizer.get_vocab()[self.pad_token]\n        self.sep_token = self.tokenizer.special_tokens_map['sep_token']\n\n        self.label_to_id = {} if target_vocab is None else target_vocab\n        self.instances = []\n\n    def get_target_size(self):\n        return len(set(self.label_to_id.values()))\n\n    def get_target_vocab(self):\n        return self.label_to_id\n\n    def __len__(self):\n        return len(self.instances)\n\n    def __getitem__(self, item):\n        return self.instances[item]\n\n    def read_data(self, data):\n        dataset_name = data if isinstance(data, str) else 'dataframe'\n        logger.info('Reading file {}'.format(dataset_name))\n        instance_idx = 0\n\n        for fields, metadata in get_ner_reader(data=data):\n            if self._max_instances != -1 and instance_idx > self._max_instances:\n                break\n            sentence_str, tokens_sub_rep, token_masks_rep, coded_ner_, gold_spans_, mask = self.parse_line_for_ner(fields=fields)\n\n            tokens_tensor = torch.tensor(tokens_sub_rep, dtype=torch.long)\n            tag_tensor = torch.tensor(coded_ner_, dtype=torch.long).unsqueeze(0)\n            token_masks_rep = torch.tensor(token_masks_rep)\n            mask_rep = torch.tensor(mask)\n\n            self.instances.append((tokens_tensor, mask_rep, token_masks_rep, gold_spans_, tag_tensor))\n            instance_idx += 1\n        logger.info('Finished reading {:d} instances from file {}'.format(len(self.instances), dataset_name))\n\n    def parse_line_for_ner(self, fields):\n        tokens_, ner_tags = fields[0], fields[-1]\n        sentence_str, tokens_sub_rep, ner_tags_rep, token_masks_rep, mask = self.parse_tokens_for_ner(tokens_, ner_tags)\n        gold_spans_ = extract_spans(ner_tags_rep)\n        coded_ner_ = [self.label_to_id[tag] if tag in self.label_to_id else self.label_to_id['O'] for tag in ner_tags_rep]\n\n        return sentence_str, tokens_sub_rep, token_masks_rep, coded_ner_, gold_spans_, mask\n\n    def parse_tokens_for_ner(self, tokens_, ner_tags):\n        sentence_str = ''\n        tokens_sub_rep, ner_tags_rep = [self.pad_token_id], ['O']\n        token_masks_rep = [False]\n        for idx, token in enumerate(tokens_):\n            if self._max_length != -1 and len(tokens_sub_rep) > self._max_length:\n                break\n            sentence_str += ' ' + ' '.join(self.tokenizer.tokenize(token.lower()))\n            rep_ = self.tokenizer(token.lower())['input_ids']\n            rep_ = rep_[1:-1]\n            tokens_sub_rep.extend(rep_)\n\n            # if we have a NER here, in the case of B, the first NER tag is the B tag, the rest are I tags.\n            ner_tag = ner_tags[idx]\n            tags, masks = _assign_ner_tags(ner_tag, rep_)\n\n            ner_tags_rep.extend(tags)\n            token_masks_rep.extend(masks)\n\n        tokens_sub_rep.append(self.pad_token_id)\n        ner_tags_rep.append('O')\n        token_masks_rep.append(False)\n        mask = [True] * len(tokens_sub_rep)\n        return sentence_str, tokens_sub_rep, ner_tags_rep, token_masks_rep, mask\n\n\nclass CoNLLUntokenizedReader(Dataset):\n    def __init__(self, max_instances=-1, max_length=50, target_vocab=None):\n        self._max_instances = max_instances\n        self._max_length = max_length\n\n        self.label_to_id = {} if target_vocab is None else target_vocab\n        self.instances = []\n\n    def __len__(self):\n        return len(self.instances)\n\n    def __getitem__(self, item):\n        return self.instances[item]\n\n    def read_data(self, data):\n        dataset_name = data if isinstance(data, str) else 'dataframe'\n        logger.info('Reading file {}'.format(dataset_name))\n        instance_idx = 0\n\n        for fields, metadata in get_ner_reader(data=data):\n            if self._max_instances != -1 and instance_idx > self._max_instances:\n                break\n            sentence_str, tags, gold_spans = self.parse_line_for_ner(fields=fields)\n\n            self.instances.append((sentence_str, tags, gold_spans))\n            instance_idx += 1\n        logger.info('Finished reading {:d} instances from file {}'.format(len(self.instances), dataset_name))\n\n    def parse_line_for_ner(self, fields):\n        tokens_, ner_tags = fields[0], fields[-1]\n        sentence, tags = self.parse_tokens_for_ner(tokens_, ner_tags)\n        gold_spans_ = extract_spans(tags)\n\n        return sentence, tags, gold_spans_\n\n    def parse_tokens_for_ner(self, tokens_, ner_tags):\n        sentence_str = ''\n        ner_tags_rep = []\n        for idx, token in enumerate(tokens_):\n            if self._max_length != -1 and len(ner_tags_rep) > self._max_length:\n                break\n            sentence_str += ' {}'.format(token)\n            ner_tags_rep.append(ner_tags[idx])\n        return sentence_str, ner_tags_rep","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:41:03.318421Z","iopub.execute_input":"2024-04-25T12:41:03.319377Z","iopub.status.idle":"2024-04-25T12:41:03.356440Z","shell.execute_reply.started":"2024-04-25T12:41:03.319338Z","shell.execute_reply":"2024-04-25T12:41:03.354787Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import gzip\nimport itertools\n\n\ndef get_ner_reader(data):\n    fin = gzip.open(data, 'rt') if data.endswith('.gz') else open(data, 'rt')\n    for is_divider, lines in itertools.groupby(fin, _is_divider):\n        if is_divider:\n            continue\n        lines = [line.strip().replace('\\u200d', '').replace('\\u200c', '').replace('\\u200b', '') for line in lines]\n\n        metadata = lines[0].strip() if lines[0].strip().startswith('# id') else None\n        fields = [line.split() for line in lines if not line.startswith('# id')]\n        fields = [list(field) for field in zip(*fields)]\n\n        yield fields, metadata\n\n\ndef _assign_ner_tags(ner_tag, rep_):\n    '''\n    Changing the token_masks so that only the first sub_word of a token has a True value, while the rest is False. This will be used for storing the predictions.\n    :param ner_tag:\n    :param rep_:\n    :return:\n    '''\n    ner_tags_rep = []\n\n    sub_token_len = len(rep_)\n    mask_ = [False] * sub_token_len\n\n    if len(mask_):\n        mask_[0] = True\n\n    if ner_tag[0] == 'B':\n        in_tag = 'I' + ner_tag[1:]\n\n        ner_tags_rep.append(ner_tag)\n        ner_tags_rep.extend([in_tag] * (sub_token_len - 1))\n    else:\n        ner_tags_rep.extend([ner_tag] * sub_token_len)\n    return ner_tags_rep, mask_\n\n\ndef extract_spans(tags):\n    cur_tag = None\n    cur_start = None\n    gold_spans = {}\n\n    def _save_span(_cur_tag, _cur_start, _cur_id, _gold_spans):\n        if _cur_start is None:\n            return _gold_spans\n        _gold_spans[(_cur_start, _cur_id - 1)] = _cur_tag  # inclusive start & end, accord with conll-coref settings\n        return _gold_spans\n\n    # iterate over the tags\n    for _id, nt in enumerate(tags):\n        indicator = nt[0]\n        if indicator == 'B':\n            gold_spans = _save_span(cur_tag, cur_start, _id, gold_spans)\n            cur_start = _id\n            cur_tag = nt[2:]\n            pass\n        elif indicator == 'I':\n            # do nothing\n            pass\n        elif indicator == 'O':\n            gold_spans = _save_span(cur_tag, cur_start, _id, gold_spans)\n            cur_tag = 'O'\n            cur_start = _id\n            pass\n    _save_span(cur_tag, cur_start, _id + 1, gold_spans)\n    return gold_spans\n\n\ndef _is_divider(line: str) -> bool:\n    empty_line = line.strip() == ''\n    if empty_line:\n        return True\n\n    first_token = line.split()[0]\n    if first_token == \"-DOCSTART-\":  # or line.startswith('# id'):  # pylint: disable=simplifiable-if-statement\n        return True\n\n    return False\n\n\ndef get_tags(tokens, tags, tokenizer=None, start_token_pattern='▁'):\n    tag_results = [], []\n    index = 0\n    tokens = tokenizer.convert_ids_to_tokens(tokens)\n    for token, tag in zip(tokens, tags):\n        if token == tokenizer.pad_token:\n            continue\n\n        if index == 0:\n            tag_results.append(tag)\n\n        elif token.startswith(start_token_pattern) and token != '▁́':\n            tag_results.append(tag)\n        index += 1\n\n    return tag_results","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:41:06.187255Z","iopub.execute_input":"2024-04-25T12:41:06.187743Z","iopub.status.idle":"2024-04-25T12:41:06.213141Z","shell.execute_reply.started":"2024-04-25T12:41:06.187704Z","shell.execute_reply":"2024-04-25T12:41:06.211664Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport os\nimport time\n\nimport pandas as pd\nimport torch\nfrom pytorch_lightning import seed_everything\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, EarlyStopping\n\n\n\nconll_iob = {'B-ORG': 0, 'I-ORG': 1, 'B-MISC': 2, 'I-MISC': 3, 'B-LOC': 4, 'I-LOC': 5, 'B-PER': 6, 'I-PER': 7, 'O': 8}\nwnut_iob = {'B-CORP': 0, 'I-CORP': 1, 'B-CW': 2, 'I-CW': 3, 'B-GRP': 4, 'I-GRP': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-PER': 8, 'I-PER': 9, 'B-PROD': 10, 'I-PROD': 11, 'O': 12}\nresume_iob = {'M-RACE': 0, 'B-PRO': 1, 'S-ORG': 2, 'B-LOC': 3, 'B-CONT': 4, 'M-CONT': 5, 'E-LOC': 6, 'M-PRO': 7, 'M-LOC': 8, 'M-TITLE': 9, 'B-ORG': 10, 'M-ORG': 11, 'E-ORG': 12,\n              'E-RACE': 13, 'B-EDU': 14, 'S-NAME': 15, 'B-TITLE': 16, 'S-RACE': 17, 'B-NAME': 18, 'B-RACE': 19, 'E-NAME': 20, 'O': 21, 'E-CONT': 22, 'M-EDU': 23, 'E-TITLE': 24, 'E-EDU': 25,\n              'M-NAME': 26, 'E-PRO': 27}\nweibo_iob = {'O': 0, 'B-PER.NOM': 1, 'E-PER.NOM': 2, 'B-LOC.NAM': 3, 'E-LOC.NAM': 4, 'B-PER.NAM': 5, 'M-PER.NAM': 6, 'E-PER.NAM': 7, 'S-PER.NOM': 8, 'B-GPE.NAM': 9, 'E-GPE.NAM': 10,\n             'B-ORG.NAM': 11, 'M-ORG.NAM': 12, 'E-ORG.NAM': 13, 'M-PER.NOM': 14, 'S-GPE.NAM': 15, 'B-ORG.NOM': 16, 'E-ORG.NOM': 17, 'M-LOC.NAM': 18, 'M-ORG.NOM': 19, 'B-LOC.NOM': 20,\n             'M-LOC.NOM': 21, 'E-LOC.NOM': 22, 'B-GPE.NOM': 23, 'E-GPE.NOM': 24, 'M-GPE.NAM': 25, 'S-PER.NAM': 26, 'S-LOC.NOM': 27}\nmsra_iob = {'O': 0, 'S-NS': 1, 'B-NS': 2, 'E-NS': 3, 'B-NT': 4, 'M-NT': 5, 'E-NT': 6, 'M-NS': 7, 'B-NR': 8, 'M-NR': 9, 'E-NR': 10, 'S-NR': 11, 'S-NT': 12}\nontonotes_iob = {'E-PER': 0, 'E-GPE': 1, 'E-LOC': 2, 'M-ORG': 3, 'E-ORG': 4, 'S-ORG': 5, 'B-GPE': 6, 'O': 7, 'M-PER': 8, 'M-LOC': 9, 'B-PER': 10, 'M-GPE': 11, 'S-LOC': 12, 'B-ORG': 13,\n                 'S-PER': 14, 'B-LOC': 15, 'S-GPE': 16}\n\n\ndef parse_args():\n    p = argparse.ArgumentParser(description='Model configuration.', add_help=False)\n    p.add_argument('--train', type=str, help='Path to the train data.', default=None)\n    p.add_argument('--test', type=str, help='Path to the test data.', default=None)\n    p.add_argument('--dev', type=str, help='Path to the dev data.', default=None)\n\n    p.add_argument('--out_dir', type=str, help='Output directory.', default='.')\n    p.add_argument('--iob_tagging', type=str, help='IOB tagging scheme', default='wnut')\n\n    p.add_argument('--max_instances', type=int, help='Maximum number of instances', default=-1)\n    p.add_argument('--max_length', type=int, help='Maximum number of tokens per instance.', default=50)\n\n    p.add_argument('--encoder_model', type=str, help='Pretrained encoder model to use', default='xlm-roberta-large')\n    p.add_argument('--model', type=str, help='Model path.', default=None)\n    p.add_argument('--model_name', type=str, help='Model name.', default=None)\n    p.add_argument('--stage', type=str, help='Training stage', default='fit')\n    p.add_argument('--prefix', type=str, help='Prefix for storing evaluation files.', default='test')\n\n    p.add_argument('--batch_size', type=int, help='Batch size.', default=128)\n    p.add_argument('--gpus', type=int, help='Number of GPUs.', default=1)\n    p.add_argument('--cuda', type=str, help='Cuda Device', default='cuda:0')\n    p.add_argument('--epochs', type=int, help='Number of epochs for training.', default=5)\n    p.add_argument('--lr', type=float, help='Learning rate', default=1e-5)\n    p.add_argument('--dropout', type=float, help='Dropout rate', default=0.1)\n\n    return p.parse_args()\n\n\ndef get_tagset(tagging_scheme):\n    if os.path.isfile(tagging_scheme):\n        # read the tagging scheme from a file\n        sep = '\\t' if tagging_scheme.endswith('.tsv') else ','\n        df = pd.read_csv(tagging_scheme, sep=sep)\n        tags = {row['tag']: row['idx'] for idx, row in df.iterrows()}\n        return tags\n\n    if 'conll' in tagging_scheme:\n        return conll_iob\n    elif 'wnut' in tagging_scheme:\n        return wnut_iob\n    elif 'resume' in tagging_scheme:\n        return resume_iob\n    elif 'ontonotes' in tagging_scheme:\n        return ontonotes_iob\n    elif 'msra' in tagging_scheme:\n        return msra_iob\n    elif 'weibo' in tagging_scheme:\n        return weibo_iob\n\n\ndef get_out_filename(out_dir, model, prefix):\n    model_name = os.path.basename(model)\n    model_name = model_name[:model_name.rfind('.')]\n    return '{}/{}_base_{}.tsv'.format(out_dir, prefix, model_name)\n\n\ndef write_eval_performance(eval_performance, out_file):\n    outstr = ''\n    added_keys = set()\n    for out_ in eval_performance:\n        for k in out_:\n            if k in added_keys or k in ['results', 'predictions']:\n                continue\n            outstr = outstr + '{}\\t{}\\n'.format(k, out_[k])\n            added_keys.add(k)\n\n    open(out_file, 'wt').write(outstr)\n    logger.info('Finished writing evaluation performance for {}'.format(out_file))\n\n\ndef get_reader(file_path, max_instances=-1, max_length=50, target_vocab=None, encoder_model='xlm-roberta-large'):\n    if file_path is None:\n        return None\n    reader = CoNLLReader(max_instances=max_instances, max_length=max_length, target_vocab=target_vocab, encoder_model=encoder_model)\n    reader.read_data(file_path)\n\n    return reader\n\n\ndef create_model(train_data, dev_data, tag_to_id, batch_size=64, dropout_rate=0.1, stage='fit', lr=1e-5, encoder_model='xlm-roberta-large', num_gpus=1):\n    return NERBaseAnnotator(train_data=train_data, dev_data=dev_data, tag_to_id=tag_to_id, batch_size=batch_size, stage=stage, encoder_model=encoder_model,\n                            dropout_rate=dropout_rate, lr=lr, pad_token_id=train_data.pad_token_id, num_gpus=num_gpus)\n\n\ndef load_model(model_file, tag_to_id=None, stage='test'):\n    if ~os.path.isfile(model_file):\n        model_file = get_models_for_evaluation(model_file)\n\n    hparams_file = model_file[:model_file.rindex('checkpoints/')] + '/hparams.yaml'\n    model = NERBaseAnnotator.load_from_checkpoint(model_file, hparams_file=hparams_file, stage=stage, tag_to_id=tag_to_id)\n    model.stage = stage\n    return model, model_file\n\n\ndef save_model(trainer, out_dir, model_name='', timestamp=None):\n    out_dir = out_dir + '/lightning_logs/version_' + str(trainer.logger.version) + '/checkpoints/'\n    if timestamp is None:\n        timestamp = time.time()\n    os.makedirs(out_dir, exist_ok=True)\n\n    outfile = out_dir + '/' + model_name + '_timestamp_' + str(timestamp) + '_final.ckpt'\n    trainer.save_checkpoint(outfile, weights_only=True)\n\n    logger.info('Stored model {}.'.format(outfile))\n    return outfile\n\n\ndef train_model(model, out_dir='', epochs=10, gpus=1):\n    trainer = get_trainer(gpus=gpus, out_dir=out_dir, epochs=epochs)\n    trainer.fit(model)\n    return trainer\n\n\ndef get_trainer(gpus=4, is_test=False, out_dir=None, epochs=10):\n    seed_everything(42)\n    c = 'gpu' if gpus > 0 else 'cpu'\n    gpu = 1 if gpus == 0 else gpus\n    if is_test:\n        return trainer(accelerator=c,devices=gpus) if torch.cuda.is_available() else pl.Trainer(val_check_interval=100)\n\n    if torch.cuda.is_available():\n        trainer = Trainer(accelerator=c,devices=gpus, deterministic=True, max_epochs=epochs,\n                             default_root_dir=out_dir, strategy='auto')\n        trainer.callbacks.append(get_lr_logger())\n    else:\n        trainer = Trainer(max_epochs=epochs, default_root_dir=out_dir)\n\n    return trainer\n\n\ndef get_lr_logger():\n    lr_monitor = LearningRateMonitor(logging_interval='step')\n    return lr_monitor\n\n\ndef get_model_earlystopping_callback():\n    es_clb = EarlyStopping(\n        monitor='val_P@OtherPRO',\n        min_delta=0.001,\n        patience=3,\n        verbose=True,\n        mode='min'\n    )\n    return es_clb\n\n\ndef get_models_for_evaluation(path):\n    if 'checkpoints' not in path:\n        path = path + '/checkpoints/'\n    model_files = list_files(path)\n    models = [f for f in model_files if f.endswith('final.ckpt')]\n\n    return models[0] if len(models) != 0 else None\n\n\ndef list_files(in_dir):\n    files = []\n    for r, d, f in os.walk(in_dir):\n        for file in f:\n            files.append(os.path.join(r, file))\n    return files","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:41:08.077293Z","iopub.execute_input":"2024-04-25T12:41:08.078433Z","iopub.status.idle":"2024-04-25T12:41:08.129968Z","shell.execute_reply.started":"2024-04-25T12:41:08.078376Z","shell.execute_reply":"2024-04-25T12:41:08.128172Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning.core as pl","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:41:11.234085Z","iopub.execute_input":"2024-04-25T12:41:11.234544Z","iopub.status.idle":"2024-04-25T12:41:11.240478Z","shell.execute_reply.started":"2024-04-25T12:41:11.234513Z","shell.execute_reply":"2024-04-25T12:41:11.238818Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from typing import List, Any\nfrom itertools import compress\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom allennlp.modules import ConditionalRandomField\nfrom allennlp.modules.conditional_random_field import allowed_transitions\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup, AutoModel\n\n\nclass NERBaseAnnotator(pl.LightningModule):\n    def __init__(self,\n                 train_data=None,\n                 dev_data=None,\n                 lr=1e-5,\n                 dropout_rate=0.1,\n                 batch_size=16,\n                 tag_to_id=None,\n                 stage='fit',\n                 pad_token_id=1,\n                 encoder_model='xlm-roberta-large',\n                 num_gpus=1):\n        super(NERBaseAnnotator, self).__init__()\n\n        self.train_data = train_data\n        self.dev_data = dev_data\n\n        self.id_to_tag = {v: k for k, v in tag_to_id.items()}\n        self.tag_to_id = tag_to_id\n        self.batch_size = batch_size\n\n        self.stage = stage\n        self.num_gpus = num_gpus\n        self.target_size = len(self.id_to_tag)\n\n        # set the default baseline model here\n        self.pad_token_id = pad_token_id\n\n        self.encoder_model = encoder_model\n        self.encoder = AutoModel.from_pretrained(encoder_model, return_dict=True)\n\n        self.feedforward = nn.Linear(in_features=self.encoder.config.hidden_size, out_features=self.target_size)\n\n        self.crf_layer = ConditionalRandomField(num_tags=self.target_size, constraints=allowed_transitions(constraint_type=\"BIO\", labels=self.id_to_tag))\n\n        self.lr = lr\n        self.dropout = nn.Dropout(dropout_rate)\n        self.outputs = []\n        self.predicts = []\n        self.span_f1 = SpanF1()\n        self.setup_model(self.stage)\n        self.save_hyperparameters('pad_token_id', 'encoder_model')\n\n    def setup_model(self, stage_name):\n        if stage_name == 'fit' and self.train_data is not None:\n            # Calculate total steps\n            train_batches = len(self.train_data) // (self.batch_size * self.num_gpus)\n            self.total_steps = 50 * train_batches\n\n            self.warmup_steps = int(self.total_steps * 0.01)\n\n    def collate_batch(self, batch):\n        batch_ = list(zip(*batch))\n        tokens, masks, token_masks, gold_spans, tags = batch_[0], batch_[1], batch_[2], batch_[3], batch_[4]\n\n        max_len = max([len(token) for token in tokens])\n        token_tensor = torch.empty(size=(len(tokens), max_len), dtype=torch.long).fill_(self.pad_token_id)\n        tag_tensor = torch.empty(size=(len(tokens), max_len), dtype=torch.long).fill_(self.tag_to_id['O'])\n        mask_tensor = torch.zeros(size=(len(tokens), max_len), dtype=torch.bool)\n        token_masks_tensor = torch.zeros(size=(len(tokens), max_len), dtype=torch.bool)\n\n        for i in range(len(tokens)):\n            tokens_ = tokens[i]\n            seq_len = len(tokens_)\n\n            token_tensor[i, :seq_len] = tokens_\n            tag_tensor[i, :seq_len] = tags[i]\n            mask_tensor[i, :seq_len] = masks[i]\n            token_masks_tensor[i, :seq_len] = token_masks[i]\n\n        return token_tensor, tag_tensor, mask_tensor, token_masks_tensor, gold_spans\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n        if self.stage == 'fit':\n            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.total_steps)\n            scheduler = {\n                'scheduler': scheduler,\n                'interval': 'step',\n                'frequency': 1\n            }\n            return [optimizer], [scheduler]\n        return [optimizer]\n\n    def train_dataloader(self):\n        loader = DataLoader(self.train_data, batch_size=self.batch_size, collate_fn=self.collate_batch, num_workers=10)\n        return loader\n\n    def val_dataloader(self):\n        if self.dev_data is None:\n            return None\n        loader = DataLoader(self.dev_data, batch_size=self.batch_size, collate_fn=self.collate_batch, num_workers=10)\n        return loader\n\n    def on_test_epoch_end(self):\n        pred_results = self.span_f1.get_metric()\n        outputs = self.outputs\n        avg_loss = np.mean([preds['loss'].item() for preds in outputs])\n        self.log_metrics(pred_results, loss=avg_loss, on_step=False, on_epoch=True)\n        out = {\"test_loss\": avg_loss, \"results\": pred_results}\n        return out\n    \n#     def test_epoch_end(self, outputs):\n#         pred_results = self.span_f1.get_metric()\n#         avg_loss = np.mean([preds['loss'].item() for preds in outputs])\n#         self.log_metrics(pred_results, loss=avg_loss, on_step=False, on_epoch=True)\n\n#         out = {\"test_loss\": avg_loss, \"results\": pred_results}\n#         return out\n\n#     def training_epoch_end(self, outputs: List[Any]) -> None:\n#         pred_results = self.span_f1.get_metric(True)\n#         avg_loss = np.mean([preds['loss'].item() for preds in outputs])\n#         self.log_metrics(pred_results, loss=avg_loss, suffix='', on_step=False, on_epoch=True)\n        \n    def on_train_epoch_end(self) -> None:\n        outputs = self.outputs\n        pred_results = self.span_f1.get_metric(True)\n        avg_loss = np.mean([preds['loss'].item() for preds in outputs])\n        self.log_metrics(pred_results, loss=avg_loss, suffix='', on_step=False, on_epoch=True)\n        self.outputs = []\n\n    def on_validation_epoch_end(self) -> None:\n        outputs = self.predicts\n        pred_results = self.span_f1.get_metric(True)\n        avg_loss = np.mean([preds['loss'].item() for preds in outputs])\n        self.log_metrics(pred_results, loss=avg_loss, suffix='val_', on_step=False, on_epoch=True)\n\n    def validation_step(self, batch, batch_idx):\n        output = self.perform_forward_step(batch)\n        self.log_metrics(output['results'], loss=output['loss'], suffix='val_', on_step=True, on_epoch=False)\n        self.predicts.append(output)\n        return output\n\n    def training_step(self, batch, batch_idx):\n        output = self.perform_forward_step(batch)\n        self.log_metrics(output['results'], loss=output['loss'], suffix='', on_step=True, on_epoch=False)\n        self.outputs.append(output)\n        return output\n\n    def test_step(self, batch, batch_idx):\n        output = self.perform_forward_step(batch, mode=self.stage)\n        self.log_metrics(output['results'], loss=output['loss'], suffix='_t', on_step=True, on_epoch=False)\n        self.outputs.append(output)\n        return output\n\n    def log_metrics(self, pred_results, loss=0.0, suffix='', on_step=False, on_epoch=True):\n        for key in pred_results:\n            self.log(suffix + key, pred_results[key], on_step=on_step, on_epoch=on_epoch, prog_bar=True, logger=True)\n\n        self.log(suffix + 'loss', loss, on_step=on_step, on_epoch=on_epoch, prog_bar=True, logger=True)\n\n    def perform_forward_step(self, batch, mode=''):\n        tokens, tags, mask, token_mask, metadata = batch\n        batch_size = tokens.size(0)\n\n        embedded_text_input = self.encoder(input_ids=tokens, attention_mask=mask)\n        embedded_text_input = embedded_text_input.last_hidden_state\n        embedded_text_input = self.dropout(F.leaky_relu(embedded_text_input))\n\n        # project the token representation for classification\n        token_scores = self.feedforward(embedded_text_input)\n        token_scores = F.log_softmax(token_scores, dim=-1)\n\n        # compute the log-likelihood loss and compute the best NER annotation sequence\n        output = self._compute_token_tags(token_scores=token_scores, mask=mask, tags=tags, metadata=metadata, batch_size=batch_size, mode=mode)\n        return output\n\n    def _compute_token_tags(self, token_scores, mask, tags, metadata, batch_size, mode=''):\n        # compute the log-likelihood loss and compute the best NER annotation sequence\n        #print(type(token_scores), type(tags), token_scores.dtype, tags.dtype)\n        loss = -self.crf_layer(token_scores, tags, mask) / float(batch_size)\n        best_path = self.crf_layer.viterbi_tags(token_scores, mask)\n\n        pred_results, pred_tags = [], []\n        for i in range(batch_size):\n            tag_seq, _ = best_path[i]\n            pred_tags.append([self.id_to_tag[x] for x in tag_seq])\n            pred_results.append(extract_spans([self.id_to_tag[x] for x in tag_seq if x in self.id_to_tag]))\n\n        self.span_f1(pred_results, metadata)\n        output = {\"loss\": loss, \"results\": self.span_f1.get_metric()}\n\n        if mode == 'predict':\n            output['token_tags'] = pred_tags\n        return output\n\n    def predict_tags(self, batch, device='cuda:0'):\n        tokens, tags, mask, metadata, token_mask = batch\n        print(type(tokens), type(tags), type(mask), type(metadata), type(token_mask))\n        tokens, mask, token_mask, tags = tokens.to(device), mask.to(device), token_mask.to(device), tags.to(device)\n        batch = tokens, tags, mask, token_mask, metadata\n\n        pred_tags = self.perform_forward_step(batch, mode='predict')['token_tags']\n        tag_results = [compress(pred_tags_, mask_) for pred_tags_, mask_ in zip(pred_tags, token_mask)]\n        return tag_results","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:41:14.675257Z","iopub.execute_input":"2024-04-25T12:41:14.676089Z","iopub.status.idle":"2024-04-25T12:41:14.728603Z","shell.execute_reply.started":"2024-04-25T12:41:14.676033Z","shell.execute_reply":"2024-04-25T12:41:14.727258Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import time\n\nclass sg:\n    out_dir = '/kaggle/working/'\n    model_name = 'xlmr_ner'\n    gpus = 1\n    epochs = 20\n    encoder_model = 'google-bert/bert-base-multilingual-cased'\n    batch_size = 64\n    lr = 0.0001\n    iob_tagging = 'wnut'\n    train = '/kaggle/input/multiconer/multiconer2022/MULTI_Multilingual/multi_train.conll'\n    dev = '/kaggle/input/multiconer/multiconer2022/MULTI_Multilingual/multi_dev.conll'\n    max_instances = 1\n    max_length = 128\n    dropout = 0.1\n    stage='fit'\n    \nif __name__ == '__main__':\n    timestamp = time.time()\n\n    out_dir_path = sg.out_dir + '/' + sg.model_name\n\n    # load the dataset first\n    train_data = get_reader(file_path=sg.train, target_vocab=get_tagset(sg.iob_tagging), encoder_model=sg.encoder_model, max_instances=sg.max_instances, max_length=sg.max_length)\n    dev_data = get_reader(file_path=sg.dev, target_vocab=get_tagset(sg.iob_tagging), encoder_model=sg.encoder_model, max_instances=sg.max_instances, max_length=sg.max_length)\n\n    model = create_model(train_data=train_data, dev_data=dev_data, tag_to_id=train_data.get_target_vocab(),\n                         dropout_rate=sg.dropout, batch_size=sg.batch_size, stage=sg.stage, lr=sg.lr,\n                         encoder_model=sg.encoder_model, num_gpus=sg.gpus)\n\n    trainer = train_model(model=model, out_dir=out_dir_path, epochs=sg.epochs)\n\n    # use pytorch lightnings saver here.\n    out_model_path = save_model(trainer=trainer, out_dir=out_dir_path, model_name=sg.model_name, timestamp=timestamp)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:41:18.784766Z","iopub.execute_input":"2024-04-25T12:41:18.785231Z","iopub.status.idle":"2024-04-25T12:47:00.318062Z","shell.execute_reply.started":"2024-04-25T12:41:18.785198Z","shell.execute_reply":"2024-04-25T12:47:00.316457Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ad1a4bd8c145939de1e34c10db306b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"336e48bb15be4915b2372062d7b6a75c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ead1ef51b5fb41aba188055385dbe418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4adf104fa4ec44299225eecbf807ca18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d4ce2074afc45548458492f675a3cd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c0f2d440dc4cd58416c25c421b64d2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"class sg:\n    out_dir = '/kaggle/working/'\n    model = '/kaggle/working/xlmr_ner/lightning_logs/version_5/checkpoints/'\n    gpus = 1\n    epochs = 20\n    encoder_model = 'google-bert/bert-base-multilingual-cased'\n    batch_size = 64\n    lr = 0.0001\n    prefix = 'bert_ner'\n    iob_tagging = 'wnut'\n    test = '/kaggle/input/multiconer/multiconer2022/MULTI_Multilingual/multi_train.conll'\n    train = '/kaggle/input/multiconer/multiconer2022/MULTI_Multilingual/multi_train.conll'\n    dev = '/kaggle/input/multiconer/multiconer2022/MULTI_Multilingual/multi_dev.conll'\n    max_instances = 1\n    max_length = 128\n    dropout = 0.1\n    stage='fit'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    timestamp = time.time()\n    #sg = parse_args()\n\n    # load the dataset first\n    test_data = get_reader(file_path=sg.test, target_vocab=get_tagset(sg.iob_tagging), max_instances=sg.max_instances, max_length=sg.max_length, encoder_model=sg.encoder_model)\n\n    model, model_file = load_model(sg.model, tag_to_id=get_tagset(sg.iob_tagging))\n    trainer = get_trainer(is_test=True)\n    out = trainer.test(model, dataloaders=DataLoader(test_data, batch_size=sg.batch_size, collate_fn=model.collate_batch))\n\n    # use pytorch lightnings saver here.\n    eval_file = get_out_filename(sg.out_dir, model_file, prefix=sg.prefix)\n    write_eval_performance(out, eval_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"python -m ner_baseline.evaluate --test test.txt --out_dir . --gpus 1 --encoder_model xlm-roberta-base \\\n                                --model MODEL_FILE_PATH --prefix xlmr_ner_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# Specify the path of the folder you want to delete\nfolder_path = \"/kaggle/working/xlmr_ner\"\n\n# Check if the folder exists before attempting to delete\nif os.path.exists(folder_path):\n    # Delete the folder and its contents recursively\n    shutil.rmtree(folder_path)\n    print(f\"{folder_path} has been deleted successfully.\")\nelse:\n    print(f\"The folder {folder_path} does not exist.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = get_reader(file_path='/kaggle/input/multiconer/multiconer2023/DE-German/de_test.conll',target_vocab=get_tagset(sg.iob_tagging), encoder_model=sg.encoder_model, max_instances=sg.max_instances, max_length=sg.max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_data[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loader = DataLoader(train_data, batch_size=2, num_workers=1,collate_fn=model.collate_batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = next(iter(loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata = b[4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.perform_forward_step(b,'predict')['token_tags'][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\ndef internet_connection():\n    try:\n        response = requests.get(\"https://huggingface.co/\", timeout=5)\n        return True\n    except requests.ConnectionError:\n        return False    \nif internet_connection():\n    print(\"The Internet is connected.\")\nelse:\n    print(\"The Internet is not connected.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response = requests.get(\"https://huggingface.co/\", timeout=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}